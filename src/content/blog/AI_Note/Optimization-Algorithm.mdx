---
title: Artificial Intelligence Note - Optimization Algorithm
description: A concise overview of optimization algorithms and practical tuning tips.
image: ../cover/cover_11.png
publishDate: 2025-12-03
tags:
  - Deep Learning
  - Artificial Intelligence
  - Notebook
---

优化算法使我们能够继续更新模型参数，并使损失函数的值最小化。优化算法的性能直接影响模型的训练效率，以及==有针对性地调整超参数==，以提高深度学习模型的性能。

## 优化和深度学习

一旦我们有了损失函数，我们就可以使用优化算法来尝试最小化损失。在优化中，损失函数通常被称为优化问题的**目标函数**。

### 优化的目标

|    概念     |                目标                |
| :-------: | :------------------------------: |
|  **优化**​  |         最小化目标，追求数学上的最优解          |
| **深度学习**​ | 在给定有限数据量的情况下，寻找一个能够==良好泛化==的合适模型 |

前者优化的是训练误差，而后者追求减少==泛化误差==。

|        训练误差        |        泛化误差        |
| :----------------: | :----------------: |
|  模型在**训练数据**上的误差。  |  模型在**新数据**上的误差。   |
| 模型**记忆和拟合**训练数据的能力 | 模型对新数据的**预测和泛化**能力 |
| 证明模型能够从数据中==学到规律== |   代表模型的==实用性==强    |

机器学习的目标是**降低泛化误差**，而训练误差只是实现该目标过程中的一个参考。

我们定义两个函数来模拟真实情况：

**真实风险函数**$f(x)$：
$$f(x) = x \cdot \cos(\pi x)$$
**经验风险函数**$g(x)$：
$$g(x) = f(x) + 0.2 \cdot \cos(5\pi x)$$
> 高频项模拟了==由训练数据有限而产生的采样误差或噪声==。因样本有限，经验风险函数在真实风险函数周围波动，导致其表面变得粗糙。

```python
def f(x):
	return x * torch.cos(np.pi * x)

def g(x):
	return f(x) + 0.2 * torch.cos(5 * np.pi * x)
```

![[真实风险-vs-经验风险.png]]